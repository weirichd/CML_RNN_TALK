{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation With Recurrent Neural Networks\n",
    "\n",
    "## David E. Weirich"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal:\n",
    "\n",
    "To generate new episodes of Sabrina the Teenage Witch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://sabrinatranscripts.wordpress.com/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_text_samples = [\n",
    "    'Hello world!',\n",
    "    'I love machine learning.',\n",
    "    'Four score and seven years ago',\n",
    "    'I love the whole world.'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "?Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkn = Tokenizer(num_words=32)\n",
    "\n",
    "tkn.fit_on_texts(example_text_samples)\n",
    "\n",
    "seqs = tkn.texts_to_sequences(example_text_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tkn.index_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the episodes data\n",
    "\n",
    "\n",
    "Use the Beautifulsoup library to parse out the text of each episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from requests import get\n",
    "from requests.exceptions import RequestException\n",
    "from contextlib import closing\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "def simple_get(url):\n",
    "    try:\n",
    "        with closing(get(url, stream=True)) as resp:\n",
    "            if is_good_response(resp):\n",
    "                return resp.content\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "    except RequestException as e:\n",
    "        print('Error during requests to {0} : {1}'.format(url, str(e)))\n",
    "        return None\n",
    "\n",
    "\n",
    "def is_good_response(resp):\n",
    "    content_type = resp.headers['Content-Type'].lower()\n",
    "    return (resp.status_code == 200\n",
    "            and content_type is not None\n",
    "            and content_type.find('html') > -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sabrina_url = 'https://sabrinatranscripts.wordpress.com/'\n",
    "\n",
    "sabrina_html = simple_get(sabrina_url)\n",
    "\n",
    "bs = BeautifulSoup(sabrina_html, 'html.parser')\n",
    "\n",
    "entry = bs.find(class_='entry')\n",
    "\n",
    "# Not looking for the links to the seasons.\n",
    "seasons = [ep for ep in entry.findChildren('p') if not ep.text.strip().startswith('Season')]\n",
    "\n",
    "episodes = []\n",
    "\n",
    "for season in seasons:\n",
    "    episodes += [(a['href'], a.text) for a in season.findChildren('a')]\n",
    "\n",
    "for i, (url, name) in enumerate(episodes):\n",
    "    episode_html = simple_get(url)\n",
    "\n",
    "    filename = '{:03}_{}'.format(i, ''.join(c for c in name if c.isalpha()).lower())\n",
    "\n",
    "    bs = BeautifulSoup(episode_html, 'html.parser')\n",
    "\n",
    "    print('{}/{}: {}'.format(i, len(episodes), name))\n",
    "\n",
    "    page_text = [p.text.strip() for p in bs.find_all('p')]\n",
    "\n",
    "    header_end_index = page_text.index(next(x for x in page_text if x.startswith('DISCLAIMER')))\n",
    "\n",
    "    footer_begin_index = page_text.index(next(x for x in page_text[::-1] if x.startswith('This entry was posted on')))\n",
    "\n",
    "    if not os.path.exists('episodes'):\n",
    "        os.makedirs('episodes')\n",
    "\n",
    "    with open('episodes/' + filename + '.txt', 'w') as f:\n",
    "        for p in page_text[header_end_index + 1:footer_begin_index]:\n",
    "            f.write(p + '\\n')\n",
    "\n",
    "print('Done! :)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat episodes/000_pilot.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "def clean_episode_text(text, header_length=0):\n",
    "    \"\"\"\n",
    "    Clean the text of an episode.\n",
    "    :param text:\n",
    "    :param header_length:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove any leading newlines or whitespaces at the beginning and end of the episode text.\n",
    "    text = text.strip()\n",
    "\n",
    "    # Insert a space before these punctuation marks so they get treated as a word.\n",
    "    for c in '.!?),':\n",
    "        text = text.replace(c, ' ' + c)\n",
    "\n",
    "    # Treat these two a special cases\n",
    "    text = text.replace(')', ' )')\n",
    "    text = text.replace('\\n', ' \\n ')\n",
    "\n",
    "    # If the \"cleaning\" I just did put two or more spaces next to each other, get rid of that.\n",
    "    while '  ' in text:\n",
    "        text = text.replace('  ', ' ')\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def load_data(episodes_path='episodes', sequence_length=32):\n",
    "    # Load the text of every file into a big list\n",
    "    texts = [clean_episode_text(open(os.path.join(episodes_path, f)).read()) for f in os.listdir(episodes_path)]\n",
    "\n",
    "    # Encode using the tokenizer\n",
    "    tkn = Tokenizer(num_words=50000, filters='\"#$%&*+-/:;<=>@[\\\\]^_`{|}~\\t')\n",
    "    tkn.fit_on_texts(texts)\n",
    "    seqs = tkn.texts_to_sequences(texts)\n",
    "\n",
    "    # \n",
    "    sub_texts = []\n",
    "    for seq in seqs:\n",
    "        sub_texts += [\n",
    "            (seq[i:i+sequence_length], seq[i+sequence_length], [j/len(seq) for j in range(i, i+sequence_length)])\n",
    "            for i in range(len(seq) - sequence_length)\n",
    "        ]\n",
    "\n",
    "    # The target variable is the \n",
    "    X = np.array([s[0] for s in sub_texts])\n",
    "    y = np.array([s[1] for s in sub_texts])\n",
    "\n",
    "    return X, y, tkn\n",
    "\n",
    "\n",
    "X, y, tokenizer = load_data()\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.sequences_to_texts(X[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LambdaCallback\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Embedding, Activation, Softmax\n",
    "\n",
    "\n",
    "# Build a RNN that consumes \n",
    "def build_model(vocab_size):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 128, input_length=32))\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(vocab_size))\n",
    "    model.add(Softmax())\n",
    "\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = build_model(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model to generate a random sentence.\n",
    "\n",
    "\n",
    "def random_sentence(sentence_len, temperature=1.0):\n",
    "    arr = np.random.randint(0, vocab_size, size=32)\n",
    "    result = []\n",
    "\n",
    "    for i in range(sentence_len):\n",
    "        predictions = model(np.expand_dims(arr, axis=0)).numpy().astype(np.float64)\n",
    "        predictions = np.squeeze(predictions)\n",
    "        \n",
    "        predictions += 10**-32 # TO avoid a divide by zero error in log\n",
    "        \n",
    "        predictions = np.log(predictions) / temperature\n",
    "        predictions = np.exp(predictions)\n",
    "        \n",
    "        predictions /= (predictions.sum() + 10**-20)\n",
    "        \n",
    "        next_word = np.argmax(np.random.multinomial(1, predictions, 1))\n",
    "        \n",
    "        result.append([next_word])\n",
    "        arr = np.insert(arr[1:], 31, next_word, axis=0)\n",
    "\n",
    "    sentence = tokenizer.sequences_to_texts(result)\n",
    "    sentence = ' '.join(sentence)\n",
    "\n",
    "    for c in '.!?(),':\n",
    "        sentence = sentence.replace(' ' + c, c)\n",
    "\n",
    "    sentence = sentence.replace('\\n', ' \\n ')\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import colorama\n",
    "\n",
    "\n",
    "# Call this function at the end of each epoch to see what the model is generating.\n",
    "def on_epoch_end():\n",
    "    for t in [0.1, 1.0, 10.0]:\n",
    "        print(colorama.Fore.RED + colorama.Style.BRIGHT)\n",
    "        print('Temp = {}'.format(t))\n",
    "        print(colorama.Style.RESET_ALL + colorama.Fore.BLUE)\n",
    "        print('\\n', random_sentence(50, t), '\\n')\n",
    "        print(colorama.Style.RESET_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "N_samples = 10**6\n",
    "\n",
    "\n",
    "# Add a callback to print a new example every epoch\n",
    "callbacks = [LambdaCallback(on_epoch_end=lambda _, __: on_epoch_end())]\n",
    "\n",
    "model.fit(X[:N_samples], y[:N_samples],\n",
    "          epochs=10,\n",
    "          batch_size=100,\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model.save('sabrina.hdf5')\n",
    "\n",
    "print(random_sentence(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.index_word[len(tokenizer.index_word)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9",
   "language": "python",
   "name": "python3.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
